\relax 
\providecommand\HyperFirstAtBeginDocument{\AtBeginDocument}
\HyperFirstAtBeginDocument{\ifx\hyper@anchor\@undefined
\global\let\oldcontentsline\contentsline
\gdef\contentsline#1#2#3#4{\oldcontentsline{#1}{#2}{#3}}
\global\let\oldnewlabel\newlabel
\gdef\newlabel#1#2{\newlabelxx{#1}#2}
\gdef\newlabelxx#1#2#3#4#5#6{\oldnewlabel{#1}{{#2}{#3}}}
\AtEndDocument{\ifx\hyper@anchor\@undefined
\let\contentsline\oldcontentsline
\let\newlabel\oldnewlabel
\fi}
\fi}
\global\let\hyper@last\relax 
\gdef\HyperFirstAtBeginDocument#1{#1}
\providecommand\HyField@AuxAddToFields[1]{}
\@writefile{toc}{\contentsline {section}{\numberline {1}Linear regression functions}{1}{section.1}}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.1}Estimation parameter validation}{1}{subsection.1.1}}
\newlabel{eq:estimate}{{1}{1}{Estimation parameter validation\relax }{equation.1.1}{}}
\newlabel{eq:estimate_cov}{{2}{1}{Estimation parameter validation\relax }{equation.1.2}{}}
\newlabel{eq:s2}{{3}{1}{Estimation parameter validation\relax }{equation.1.3}{}}
\newlabel{eq:error}{{4}{1}{Estimation parameter validation\relax }{equation.1.4}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.2}Regularization verification}{2}{subsection.1.2}}
\newlabel{eq:loss_function}{{8}{2}{Regularization verification\relax }{equation.1.8}{}}
\newlabel{LSestimator}{{13}{2}{Regularization verification\relax }{equation.1.13}{}}
\newlabel{LSderivative}{{14}{2}{Regularization verification\relax }{equation.1.14}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.3}Polynomial fitting validation}{2}{subsection.1.3}}
\@writefile{lof}{\contentsline {figure}{\numberline {1}{\ignorespaces Estimated function using polynomial fitting. Each sample of regressor vector $\mathbf  {x}$ is randomly drawn from uniform distribution $[0,10]$. The true function is $\mathbf  {y}=1+5\mathbf  {x} + 0.5\mathbf  {x}^2 - 0.1\mathbf  {x}^3$, the noise variance is set to 1, and the number of samples is 20. \relax }}{3}{figure.caption.1}}
\providecommand*\caption@xref[2]{\@setref\relax\@undefined{#1}}
\newlabel{fig:poly1d}{{1}{3}{Estimated function using polynomial fitting. Each sample of regressor vector $\mathbf {x}$ is randomly drawn from uniform distribution $[0,10]$. The true function is $\mathbf {y}=1+5\mathbf {x} + 0.5\mathbf {x}^2 - 0.1\mathbf {x}^3$, the noise variance is set to 1, and the number of samples is 20. \relax \relax }{figure.caption.1}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.4}(e) One dimensional model plotting}{3}{subsection.1.4}}
\@writefile{lof}{\contentsline {figure}{\numberline {2}{\ignorespaces Estimated function using linear regression (LR). Uncertainty is illustrated as the region between the two dotted lines with confidence level 0.95. Each sample of regressor vector $x$ is randomly drawn from uniform distribution $[0,10]$. The true function is $y=2+2x$, the noise variance is set to 2, and the number of samples is 20. \relax }}{3}{figure.caption.2}}
\newlabel{fig:confidence}{{2}{3}{Estimated function using linear regression (LR). Uncertainty is illustrated as the region between the two dotted lines with confidence level 0.95. Each sample of regressor vector $x$ is randomly drawn from uniform distribution $[0,10]$. The true function is $y=2+2x$, the noise variance is set to 2, and the number of samples is 20. \relax \relax }{figure.caption.2}{}}
\@writefile{toc}{\contentsline {section}{\numberline {2}KNN-regression functions}{3}{section.2}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.1}KNN v.s. Linear regression}{3}{subsection.2.1}}
\@writefile{lof}{\contentsline {figure}{\numberline {3}{\ignorespaces Estimated functions using linear regression (LR) and KNN regressor (K=7). Each sample of regressor vector $\mathbf  {x}$ is randomly drawn from uniform distribution $[0,10]$. The true function is $\mathbf  {y}=2+2\mathbf  {x}$, the noise variance is set to 1, and the number of samples is 50.\relax }}{4}{figure.caption.3}}
\newlabel{fig:KNNvsLR}{{3}{4}{Estimated functions using linear regression (LR) and KNN regressor (K=7). Each sample of regressor vector $\mathbf {x}$ is randomly drawn from uniform distribution $[0,10]$. The true function is $\mathbf {y}=2+2\mathbf {x}$, the noise variance is set to 1, and the number of samples is 50.\relax \relax }{figure.caption.3}{}}
\@writefile{toc}{\contentsline {section}{\numberline {3}Estimating one dimensional functions}{4}{section.3}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.1}Linear data}{4}{subsection.3.1}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {3.1.1}Linear regression model (constant + linear term)}{4}{subsubsection.3.1.1}}
\@writefile{lot}{\contentsline {table}{\numberline {1}{\ignorespaces Estimation results of a linear regression model with only constant and linear terms.\relax }}{4}{table.caption.4}}
\newlabel{tab:linear}{{1}{4}{Estimation results of a linear regression model with only constant and linear terms.\relax \relax }{table.caption.4}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {3.1.2}Linear regression model (5th order polynomial)}{5}{subsubsection.3.1.2}}
\@writefile{lof}{\contentsline {figure}{\numberline {4}{\ignorespaces Model quality (mean square error) v.s. Number of data\relax }}{5}{figure.caption.5}}
\newlabel{fig:3b}{{4}{5}{Model quality (mean square error) v.s. Number of data\relax \relax }{figure.caption.5}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {3.1.3}Parameter variance validation via Monte Carlo simulation}{5}{subsubsection.3.1.3}}
\@writefile{lof}{\contentsline {figure}{\numberline {5}{\ignorespaces Contour plots of parameter estimates using 10 generated data over 1000 Monte Carlo trials. In the left subplot is the theoretical result on one realization (training data) and in the left plot is the distribution of found parameter estimates on different data sets.\relax }}{5}{figure.caption.6}}
\newlabel{fig:MC}{{5}{5}{Contour plots of parameter estimates using 10 generated data over 1000 Monte Carlo trials. In the left subplot is the theoretical result on one realization (training data) and in the left plot is the distribution of found parameter estimates on different data sets.\relax \relax }{figure.caption.6}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {3.1.4}KNN models}{6}{subsubsection.3.1.4}}
\@writefile{lof}{\contentsline {figure}{\numberline {6}{\ignorespaces Model quality (mean square error) of KNN model v.s. number of neighbors and number of data. Noise variance is 1.\relax }}{6}{figure.caption.7}}
\newlabel{fig:KNNnumberofdata}{{6}{6}{Model quality (mean square error) of KNN model v.s. number of neighbors and number of data. Noise variance is 1.\relax \relax }{figure.caption.7}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {7}{\ignorespaces Model quality (mean square error) of KNN model v.s. number of neighbors and noise variance. Data size is 1000.\relax }}{6}{figure.caption.8}}
\newlabel{fig:KNNnoisevariance}{{7}{6}{Model quality (mean square error) of KNN model v.s. number of neighbors and noise variance. Data size is 1000.\relax \relax }{figure.caption.8}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.2}Polynomial data}{6}{subsection.3.2}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {3.2.1}Linear regression model (constant + linear term)}{6}{subsubsection.3.2.1}}
\@writefile{lof}{\contentsline {figure}{\numberline {8}{\ignorespaces Mean square error of linear and 5th degree polynomial.\relax }}{7}{figure.caption.9}}
\newlabel{fig:proj1-3_2a-mse}{{8}{7}{Mean square error of linear and 5th degree polynomial.\relax \relax }{figure.caption.9}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {9}{\ignorespaces Model uncertainty for 10 samples (upper left), 100 samples (upper right), 1000 samples (lower left), and 10000 samples (lower right).\relax }}{7}{figure.caption.10}}
\newlabel{fig:uncertainty_lim}{{9}{7}{Model uncertainty for 10 samples (upper left), 100 samples (upper right), 1000 samples (lower left), and 10000 samples (lower right).\relax \relax }{figure.caption.10}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {3.2.2}Linear regression model (polynomial + regularization)}{7}{subsubsection.3.2.2}}
\@writefile{lof}{\contentsline {figure}{\numberline {10}{\ignorespaces Multiple polynomials of various degree.\relax }}{8}{figure.caption.11}}
\newlabel{fig:multipoly}{{10}{8}{Multiple polynomials of various degree.\relax \relax }{figure.caption.11}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {11}{\ignorespaces Model degree vs mean squared error.\relax }}{8}{figure.caption.12}}
\newlabel{fig:mseVSdeg}{{11}{8}{Model degree vs mean squared error.\relax \relax }{figure.caption.12}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {3.2.3}Regress unsymmetrical data using linear regression}{8}{subsubsection.3.2.3}}
\@writefile{lof}{\contentsline {figure}{\numberline {12}{\ignorespaces Model quality (mean square error) v.s. Regularization term.\relax }}{9}{figure.caption.13}}
\newlabel{fig:mseVSregul}{{12}{9}{Model quality (mean square error) v.s. Regularization term.\relax \relax }{figure.caption.13}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {13}{\ignorespaces Polynomial regressors of degree 10 with varying regularization parameter.\relax }}{9}{figure.caption.14}}
\newlabel{fig:poly10}{{13}{9}{Polynomial regressors of degree 10 with varying regularization parameter.\relax \relax }{figure.caption.14}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {3.2.4}Regress unsymmetrical data using KNN}{9}{subsubsection.3.2.4}}
\@writefile{lof}{\contentsline {figure}{\numberline {14}{\ignorespaces Regressing unsymmetrical data (100 training data and 100 validation data).\relax }}{10}{figure.caption.15}}
\newlabel{fig:unsymmetrical}{{14}{10}{Regressing unsymmetrical data (100 training data and 100 validation data).\relax \relax }{figure.caption.15}{}}
\newlabel{proj1-3_2d-neighbors}{{15a}{10}{Varying number of samples, constant noise of variance 1.\relax \relax }{figure.caption.16}{}}
\newlabel{sub@proj1-3_2d-neighbors}{{a}{10}{Varying number of samples, constant noise of variance 1.\relax \relax }{figure.caption.16}{}}
\newlabel{proj1-3_2d-noise}{{15b}{10}{Varying noise, constant number of samples of 1000.\relax \relax }{figure.caption.16}{}}
\newlabel{sub@proj1-3_2d-noise}{{b}{10}{Varying noise, constant number of samples of 1000.\relax \relax }{figure.caption.16}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {15}{\ignorespaces Number of neighbors depending on noise and number of samples.\relax }}{10}{figure.caption.16}}
\newlabel{proj1-3_2d}{{15}{10}{Number of neighbors depending on noise and number of samples.\relax \relax }{figure.caption.16}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.3}Chirp data}{10}{subsection.3.3}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {3.3.1}Influences of data size and noise level on polynomial model}{10}{subsubsection.3.3.1}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {3.3.2}High-degree polynomial with regularization v.s. Low-degree polynomial without regularization}{11}{subsubsection.3.3.2}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {3.3.3}Linear regression v.s. KNN}{11}{subsubsection.3.3.3}}
\@writefile{lof}{\contentsline {figure}{\numberline {16}{\ignorespaces Model degree VS mean square error. For a KNN regressor the model degree corresponds to the number of neighbors while the degree of polynomials considered in the case of a polynomial regressor. Using a regularization factor of 10.\relax }}{11}{figure.caption.17}}
\newlabel{fig:KNNvsPoly-degree}{{16}{11}{Model degree VS mean square error. For a KNN regressor the model degree corresponds to the number of neighbors while the degree of polynomials considered in the case of a polynomial regressor. Using a regularization factor of 10.\relax \relax }{figure.caption.17}{}}
\@writefile{toc}{\contentsline {section}{\numberline {4}Estimating two and high dimensional functions}{12}{section.4}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.1}Two dimensional data}{12}{subsection.4.1}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {4.1.1}Linear regression model}{12}{subsubsection.4.1.1}}
\newlabel{fig:twoDimData1-LR}{{17a}{12}{twoDimData1\relax \relax }{figure.caption.18}{}}
\newlabel{sub@fig:twoDimData1-LR}{{a}{12}{twoDimData1\relax \relax }{figure.caption.18}{}}
\newlabel{fig:twodimData2-LR}{{17b}{12}{twoDimData2\relax \relax }{figure.caption.18}{}}
\newlabel{sub@fig:twodimData2-LR}{{b}{12}{twoDimData2\relax \relax }{figure.caption.18}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {17}{\ignorespaces Linear regression on two data sets.\relax }}{12}{figure.caption.18}}
\newlabel{fig:twoDimData-LR}{{17}{12}{Linear regression on two data sets.\relax \relax }{figure.caption.18}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {4.1.2}Polynomial models}{12}{subsubsection.4.1.2}}
\newlabel{proj1-4_1d-degree-data1}{{18a}{12}{twoDimData1\relax \relax }{figure.caption.19}{}}
\newlabel{sub@proj1-4_1d-degree-data1}{{a}{12}{twoDimData1\relax \relax }{figure.caption.19}{}}
\newlabel{proj1-4_1d-degree-data2}{{18b}{12}{twoDimData2\relax \relax }{figure.caption.19}{}}
\newlabel{sub@proj1-4_1d-degree-data2}{{b}{12}{twoDimData2\relax \relax }{figure.caption.19}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {18}{\ignorespaces Mean square error VS degree of polynomials in polynomial regression for two data sets.\relax }}{12}{figure.caption.19}}
\newlabel{fig:proj1-4_1d-degree}{{18}{12}{Mean square error VS degree of polynomials in polynomial regression for two data sets.\relax \relax }{figure.caption.19}{}}
\newlabel{fig:proj1-4_1e-mse-data1}{{19a}{13}{twoDimData1\relax \relax }{figure.caption.20}{}}
\newlabel{sub@fig:proj1-4_1e-mse-data1}{{a}{13}{twoDimData1\relax \relax }{figure.caption.20}{}}
\newlabel{fig:proj1-4_1e-mse-data2}{{19b}{13}{twoDimData2\relax \relax }{figure.caption.20}{}}
\newlabel{sub@fig:proj1-4_1e-mse-data2}{{b}{13}{twoDimData2\relax \relax }{figure.caption.20}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {19}{\ignorespaces Mean square error VS number of neighbors for a KNN regressor for two different data sets.\relax }}{13}{figure.caption.20}}
\newlabel{fig:proj1-4_1e-mse}{{19}{13}{Mean square error VS number of neighbors for a KNN regressor for two different data sets.\relax \relax }{figure.caption.20}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {4.1.3}KNN model}{13}{subsubsection.4.1.3}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.2}Ten dimensional data}{13}{subsection.4.2}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {4.2.1}Number of regressors}{13}{subsubsection.4.2.1}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {4.2.2}Testing result of linear regression model}{13}{subsubsection.4.2.2}}
\@writefile{lof}{\contentsline {figure}{\numberline {20}{\ignorespaces Regularization on linear regression models of polynomials of degree 1 and 3 given in the legend.\relax }}{14}{figure.caption.21}}
\newlabel{fig:proj1-4_2e-regul}{{20}{14}{Regularization on linear regression models of polynomials of degree 1 and 3 given in the legend.\relax \relax }{figure.caption.21}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {21}{\ignorespaces Number of neighbours for KNN regressor on 10 dimensional data.\relax }}{14}{figure.caption.22}}
\newlabel{fig:proj1-4_2e-KNN}{{21}{14}{Number of neighbours for KNN regressor on 10 dimensional data.\relax \relax }{figure.caption.22}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {4.2.3}Testing result of KNN}{14}{subsubsection.4.2.3}}
