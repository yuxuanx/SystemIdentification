\relax 
\providecommand\hyper@newdestlabel[2]{}
\providecommand\HyperFirstAtBeginDocument{\AtBeginDocument}
\HyperFirstAtBeginDocument{\ifx\hyper@anchor\@undefined
\global\let\oldcontentsline\contentsline
\gdef\contentsline#1#2#3#4{\oldcontentsline{#1}{#2}{#3}}
\global\let\oldnewlabel\newlabel
\gdef\newlabel#1#2{\newlabelxx{#1}#2}
\gdef\newlabelxx#1#2#3#4#5#6{\oldnewlabel{#1}{{#2}{#3}}}
\AtEndDocument{\ifx\hyper@anchor\@undefined
\let\contentsline\oldcontentsline
\let\newlabel\oldnewlabel
\fi}
\fi}
\global\let\hyper@last\relax 
\gdef\HyperFirstAtBeginDocument#1{#1}
\providecommand\HyField@AuxAddToFields[1]{}
\providecommand\HyField@AuxAddToCoFields[2]{}
\@writefile{toc}{\contentsline {section}{\numberline {1}Linear regression functions}{1}{section.1}}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.1}Estimation parameter validation}{1}{subsection.1.1}}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.2}Regularization verification}{1}{subsection.1.2}}
\newlabel{LSestimator}{{1}{1}{Regularization verification}{equation.1.1}{}}
\newlabel{LSderivative}{{2}{1}{Regularization verification}{equation.1.2}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.3}Polynomial fitting validation}{2}{subsection.1.3}}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.4}One dimensional model plotting}{2}{subsection.1.4}}
\@writefile{lof}{\contentsline {figure}{\numberline {1}{\ignorespaces Estimated function using linear regression (LR). Uncertainty is illustrated as shaded area with confidence level 0.95. Each sample of regressor vector $\mathbf  {x}$ is randomly drawn from uniform distribution $[0,10]$. The true function is $\mathbf  {y}=2\mathbf  {x}$, the noise variance is set to 2, and the number of samples is 20. Estimation: $\mathbf  {y} = -0.71+2.24\mathbf  {x}$; Estimation upper bound: $\mathbf  {y} = 0.81+2.12\mathbf  {x}$; Estimation lower bound: $\mathbf  {y} = -2.25+2.36\mathbf  {x}$.}}{2}{figure.1}}
\@writefile{toc}{\contentsline {section}{\numberline {2}KNN-regression functions}{3}{section.2}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.1}KNN v.s. Linear regression}{3}{subsection.2.1}}
\@writefile{lof}{\contentsline {figure}{\numberline {2}{\ignorespaces Estimated functions using linear regression (LR) and KNN regressor (K=7). Each sample of regressor vector $\mathbf  {x}$ is randomly drawn from uniform distribution $[0,10]$. The true function is $\mathbf  {y}=2\mathbf  {x}$, the noise variance is set to 1, and the number of samples is 50.}}{3}{figure.2}}
\@writefile{toc}{\contentsline {section}{\numberline {3}Estimating one dimensional functions}{3}{section.3}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.1}Linear data}{3}{subsection.3.1}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {3.1.1}Linear regression model (constant + linear term)}{3}{subsubsection.3.1.1}}
\@writefile{lot}{\contentsline {table}{\numberline {1}{\ignorespaces Estimation results of a linear regression model with only constant and linear terms.}}{3}{table.1}}
\newlabel{tab:linear}{{1}{3}{Estimation results of a linear regression model with only constant and linear terms}{table.1}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {3.1.2}Linear regression model (5th order polynomial)}{4}{subsubsection.3.1.2}}
\@writefile{lof}{\contentsline {figure}{\numberline {3}{\ignorespaces Model quality (mean square error) v.s. Number of data}}{4}{figure.3}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {3.1.3}Parameter variance validation via Monte Carlo simulation}{4}{subsubsection.3.1.3}}
\@writefile{lof}{\contentsline {figure}{\numberline {4}{\ignorespaces Histogram of parameter estimates using 1000 generated data over 1000 Monte Carlo trials.}}{4}{figure.4}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {3.1.4}KNN models}{5}{subsubsection.3.1.4}}
\@writefile{lof}{\contentsline {figure}{\numberline {5}{\ignorespaces Model quality (mean square error) of KNN model v.s. number of neighbors and number of data. Noise variance is 1.}}{5}{figure.5}}
\newlabel{fig:KNNnumberofdata}{{5}{5}{Model quality (mean square error) of KNN model v.s. number of neighbors and number of data. Noise variance is 1}{figure.5}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {6}{\ignorespaces Model quality (mean square error) of KNN model v.s. number of neighbors and noise variance. Data size is 1000.}}{5}{figure.6}}
\newlabel{fig:KNNnoisevariance}{{6}{5}{Model quality (mean square error) of KNN model v.s. number of neighbors and noise variance. Data size is 1000}{figure.6}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.2}Polynomial data}{6}{subsection.3.2}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {3.2.1}Linear regression model (constant + linear term)}{6}{subsubsection.3.2.1}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {3.2.2}Linear regression model (polynomial + regularization)}{6}{subsubsection.3.2.2}}
\@writefile{lof}{\contentsline {figure}{\numberline {7}{\ignorespaces Model quality (mean square error) v.s. Polynomial degree.}}{6}{figure.7}}
\newlabel{fig:5thorderpolynomial}{{7}{6}{Model quality (mean square error) v.s. Polynomial degree}{figure.7}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {8}{\ignorespaces Model quality (mean square error) v.s. Regularization term.}}{7}{figure.8}}
\newlabel{fig:regularization}{{8}{7}{Model quality (mean square error) v.s. Regularization term}{figure.8}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {3.2.3}Regress unsymmetrical data using linear regression}{7}{subsubsection.3.2.3}}
\@writefile{lof}{\contentsline {figure}{\numberline {9}{\ignorespaces Regressing unsymmetrical data.}}{7}{figure.9}}
\newlabel{fig:unsymmetrical}{{9}{7}{Regressing unsymmetrical data}{figure.9}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {3.2.4}Regress unsymmetrical data using KNN}{8}{subsubsection.3.2.4}}
\@writefile{lof}{\contentsline {figure}{\numberline {10}{\ignorespaces Regressing unsymmetrical data on a KNN. Noise variance is 1.}}{8}{figure.10}}
\newlabel{fig:unsymmetricalKNN1}{{10}{8}{Regressing unsymmetrical data on a KNN. Noise variance is 1}{figure.10}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {11}{\ignorespaces Regressing unsymmetrical data on a KNN. Data size is 1000.}}{8}{figure.11}}
\newlabel{fig:unsymmetricalKNN2}{{11}{8}{Regressing unsymmetrical data on a KNN. Data size is 1000}{figure.11}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.3}Chirp data}{9}{subsection.3.3}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {3.3.1}Influences of data size and noise level on polynomial model}{9}{subsubsection.3.3.1}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {3.3.2}High-degree polynomial with regularization v.s. Low-degree polynomial without regularization}{9}{subsubsection.3.3.2}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {3.3.3}Linear regression v.s. KNN}{9}{subsubsection.3.3.3}}
\@writefile{toc}{\contentsline {section}{\numberline {4}Estimating two and high dimensional functions}{9}{section.4}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.1}Two dimensional data}{9}{subsection.4.1}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {4.1.1}Linear regression model}{10}{subsubsection.4.1.1}}
\@writefile{lof}{\contentsline {figure}{\numberline {12}{\ignorespaces Regressing twoDimData1 on a linear regression model}}{10}{figure.12}}
\newlabel{fig:twodimData1}{{12}{10}{Regressing twoDimData1 on a linear regression model}{figure.12}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {13}{\ignorespaces Regressing twoDimData2 on a linear regression model}}{10}{figure.13}}
\newlabel{fig:twodimData2}{{13}{10}{Regressing twoDimData2 on a linear regression model}{figure.13}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {4.1.2}Polynomial models}{11}{subsubsection.4.1.2}}
\@writefile{lot}{\contentsline {table}{\numberline {2}{\ignorespaces Mean square error}}{11}{table.2}}
\newlabel{tab:twodimData}{{2}{11}{Mean square error}{table.2}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {4.1.3}KNN model}{11}{subsubsection.4.1.3}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.2}Ten dimensional data}{11}{subsection.4.2}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {4.2.1}Number of regressors}{11}{subsubsection.4.2.1}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {4.2.2}Testing result of linear regression model}{11}{subsubsection.4.2.2}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {4.2.3}Testing result of KNN}{11}{subsubsection.4.2.3}}
